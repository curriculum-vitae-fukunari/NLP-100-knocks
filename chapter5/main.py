'''
å¤ç›®æ¼±çŸ³ã®å°èª¬ã€å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€ã®æ–‡ç« ï¼ˆneko.txtï¼‰ã‚’CaboChaã‚’ä½¿ã£ã¦ä¿‚ã‚Šå—ã‘è§£æã—ï¼Œ
ãã®çµæœã‚’neko.txt.cabochaã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã›ã‚ˆï¼
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨ã„ã¦ï¼Œä»¥ä¸‹ã®å•ã«å¯¾å¿œã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè£…ã›ã‚ˆï¼
'''

'''
ä¿‚ã‚Šå—ã‘è§£æ
http://blog.aqutras.com/entry/2016/04/18/210000

æ–‡ç« ã‚’å½¢æ…‹ç´ ã«åˆ†ã‘ãŸå¾Œã€ä¿®é£¾é–¢ä¿‚ã®è§£æã‚’è¡Œã†ã“ã¨
èªã¨èªã®ç¹‹ãŒã‚Šã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
äººé–“ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„å½¢å¼ã§å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½
'''

'''
Cabocha
http://chasen.naist.jp/chaki/t/2005-08-29/doc/CaboCha%20Yet%20Another%20Japanese%20Dependency%20Structure%20Analyzer.htm
SVMã«åŸºã¥ãé«˜æ€§èƒ½ãªä¿‚ã‚Šå—ã‘è§£æå™¨è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚
'''

'''
ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

CaboChaã‚’å‹•ã‹ã™ãŸã‚ã«ã¯ä»¥ä¸‹ã®3ã¤ãŒå¿…è¦ã¨ãªã‚‹ã€‚
* CRF++
* MeCab
* mecab-ipadicãªã©ã®è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«

CRF++

æ¡ä»¶ä»˜ãç¢ºç‡å ´ã¨ã„ã†ã‚‚ã®ã‚’ç”¨ã„ã¦ã„ã‚‹ã‚‰ã—ã„

brew install crf++
ğŸº  /usr/local/Cellar/crf++/0.58: 12 files, 719.5KB

brew install cabocha
ğŸº  /usr/local/Cellar/cabocha/0.69: 27 files, 236MB
'''

'''
CaboChaã§å§‹ã‚ã‚‹ä¿‚ã‚Šå—ã‘è§£æ
https://qiita.com/nezuq/items/f481f07fc0576b38e81d

cabocha -f1
ä¸€éƒã¯äºŒéƒã‚’æã„ãŸçµµã‚’ä¸‰éƒã«è´ˆã£ãŸã€‚

* 0 5D 0/1 -0.620584
ä¸€éƒ  åè©,äººå,*,*,ä¸€éƒ,ã„ã¡ã‚ã†,*
ã¯ åŠ©è©,å‰¯åŠ©è©,*,*,ã¯,ã¯,*
* 1 2D 0/1 1.710282
äºŒéƒ  åè©,äººå,*,*,äºŒéƒ,ã˜ã‚ã†,*
ã‚’ åŠ©è©,æ ¼åŠ©è©,*,*,ã‚’,ã‚’,*
* 2 3D 0/0 1.594028
æã„ãŸ   å‹•è©,*,å­éŸ³å‹•è©ã‚«è¡Œ,ã‚¿å½¢,æã,ãˆãŒã„ãŸ,ä»£è¡¨è¡¨è¨˜:æã
* 3 5D 0/1 -0.620584
çµµ åè©,æ™®é€šåè©,*,*,çµµ,ãˆ,æ¼¢å­—èª­ã¿:éŸ³ ä»£è¡¨è¡¨è¨˜:çµµ
ã‚’ åŠ©è©,æ ¼åŠ©è©,*,*,ã‚’,ã‚’,*
* 4 5D 0/1 -0.620584
ä¸‰éƒ  åè©,äººå,*,*,ä¸‰éƒ,ã•ã¶ã‚ã†,*
ã« åŠ©è©,æ ¼åŠ©è©,*,*,ã«,ã«,*
* 5 -1D 0/0 0.000000
è´ˆã£ãŸ   å‹•è©,*,å­éŸ³å‹•è©ãƒ©è¡Œ,ã‚¿å½¢,è´ˆã‚‹,ãŠãã£ãŸ,ä»£è¡¨è¡¨è¨˜:è´ˆã‚‹
ã€‚ ç‰¹æ®Š,å¥ç‚¹,*,*,ã€‚,ã€‚,*
EOS

1è¡Œç›®
1. *
2. æ–‡ç¯€ç•ªå·
3. ä¿‚ã‚Šå…ˆã®æ–‡ç¯€ç•ªå·(ä¿‚ã‚Šå…ˆãªã—:-1)
4. ä¸»è¾ã®å½¢æ…‹ç´ ç•ªå·/æ©Ÿèƒ½èªã®å½¢æ…‹ç´ ç•ªå·
5. ä¿‚ã‚Šé–¢ä¿‚ã®ã‚¹ã‚³ã‚¢(å¤§ãã„æ–¹ãŒä¿‚ã‚Šã‚„ã™ã„)

2è¡Œç›®
1. è¡¨å±¤å½¢ ï¼ˆTabåŒºåˆ‡ã‚Šï¼‰
2. å“è©
3. å“è©ç´°åˆ†é¡1
4. å“è©ç´°åˆ†é¡2
5. å“è©ç´°åˆ†é¡3
6. æ´»ç”¨å½¢
7. æ´»ç”¨å‹
8. åŸå½¢
9. èª­ã¿
10. ç™ºéŸ³
'''

'''
$ cabocha -f1 neko.txt -o neko.txt.cabocha
'''

'''
40. ä¿‚ã‚Šå—ã‘è§£æçµæœã®èª­ã¿è¾¼ã¿ï¼ˆå½¢æ…‹ç´ ï¼‰
å½¢æ…‹ç´ ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹Morphã‚’å®Ÿè£…ã›ã‚ˆï¼
ã“ã®ã‚¯ãƒ©ã‚¹ã¯è¡¨å±¤å½¢ï¼ˆsurfaceï¼‰ï¼ŒåŸºæœ¬å½¢ï¼ˆbaseï¼‰ï¼Œå“è©ï¼ˆposï¼‰ï¼Œå“è©ç´°åˆ†é¡1ï¼ˆpos1ï¼‰ã‚’ãƒ¡ãƒ³ãƒå¤‰æ•°ã«æŒã¤ã“ã¨ã¨ã™ã‚‹ï¼
ã•ã‚‰ã«ï¼ŒCaboChaã®è§£æçµæœï¼ˆneko.txt.cabochaï¼‰ã‚’èª­ã¿è¾¼ã¿ï¼Œå„æ–‡ã‚’Morphã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¡¨ç¾ã—ï¼Œ
3æ–‡ç›®ã®å½¢æ…‹ç´ åˆ—ã‚’è¡¨ç¤ºã›ã‚ˆï¼
'''
from typing import List, Tuple
from chapter5.util import Chunk, Morph, ReceivedRelative, PartOfSpeech, relative_pair, modifiee_modifiers
import re


def received_relative(item_list: List[str]) -> ReceivedRelative:

    return ReceivedRelative(chunk_number=int(item_list[1]),
                            chunk_number_modifiee=int(item_list[2].strip("D")),
                            morph_number_head=int(item_list[3].split("/")[0]),
                            morph_number_function_word=int(item_list[3].split("/")[1]),
                            score_relation=float(item_list[4]))


def morph(item_list: List[str]) -> Morph:

    # "*"ã§è£œå®Œ
    while len(split_line) < 10:
        split_line.append("*")

    return Morph(surface=item_list[0],
                 pos=item_list[1],
                 pos1=item_list[2],
                 pos2=item_list[3],
                 pos3=item_list[4],
                 inflection=item_list[5],
                 conjugation=item_list[6],
                 base=item_list[7],
                 syllabary=item_list[8],
                 pronunciation=item_list[9])


with open("./chapter5/neko.txt.cabocha", "r") as file:
    lines: List[str] = file.readlines()

sequences: List[List[str]] = []
sequence: List[str] = []
for line in lines:
    # æ–‡ã®çµ‚ã‚ã‚Šã‚’è¡¨ã™EOSãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
    if line == "EOS\n":
        sequences.append(sequence)
        sequence = []
    # ä¿‚ã‚Šå—ã‘ã‹ã€å½¢æ…‹ç´ ã‚’è¡¨ã—ã¦ã„ã‚‹å ´åˆ
    else:
        sequence.append(line)

sentences: List[List[Chunk]] = []
sentence: List[Chunk] = []
for sequence in sequences:
    if len(sequence) == 0:
        continue

    morph_list: List[Morph] = []
    for index, line in enumerate(sequence):
        split_line: List[str] = re.split("[\t, ]", line.strip("\n"))

        # å½¢æ…‹ç´ ã‚’è¡¨ã™è¡Œã®å ´åˆ
        if split_line[0] != "*":
            morph_: Morph = morph(item_list=split_line)
            morph_list.append(morph_)
            continue

        # ä¿‚ã‚Šã†ã‘ã‚’è¡¨ã™è¡Œã®å ´åˆ(æ–‡ã®å§‹ã¾ã‚Šã§ã¯ãªãã€ã™ã§ã«å½¢æ…‹ç´ ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹æ™‚)
        if index != 0:
            # ãã‚Œã¾ã§ã«å‡ºç¾ã—ãŸä¿‚ã‚Šå—ã‘ã¨å½¢æ…‹ç´ ã®ãƒªã‚¹ãƒˆã‹ã‚‰æ–‡ç¯€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã¤ãã‚Šã€æ–‡ãƒªã‚¹ãƒˆã«è¿½åŠ 
            chunk = Chunk(received_relative=received_relative_, morph_list=morph_list)
            sentence.append(chunk)
            morph_list = []

        received_relative_: ReceivedRelative = received_relative(item_list=split_line)

    chunk = Chunk(received_relative=received_relative_, morph_list=morph_list)
    sentence.append(chunk)
    sentences.append(sentence)
    morph_list = []
    sentence = []


# # 8æ–‡ç›®ã®ä¸­èº«ã‚’ç¢ºèª
# for chunk in sentences[8]:
#     print(str(chunk))

'''
42. ä¿‚ã‚Šå…ƒã¨ä¿‚ã‚Šå…ˆã®æ–‡ç¯€ã®è¡¨ç¤º
ä¿‚ã‚Šå…ƒã®æ–‡ç¯€ã¨ä¿‚ã‚Šå…ˆã®æ–‡ç¯€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¿ãƒ–åŒºåˆ‡ã‚Šå½¢å¼ã§ã™ã¹ã¦æŠ½å‡ºã›ã‚ˆï¼
ãŸã ã—ï¼Œå¥èª­ç‚¹ãªã©ã®è¨˜å·ã¯å‡ºåŠ›ã—ãªã„ã‚ˆã†ã«ã›ã‚ˆï¼
'''
# for sentence in sentences:
#     pairs: List[Tuple[Chunk, Chunk]] = relative_pair(chunk_list=sentence)
#     for pair in pairs:
#         print(pair[0].surface_words(need_syntax=True) + "\t" + pair[1].surface_words(need_syntax=True))

'''
43. åè©ã‚’å«ã‚€æ–‡ç¯€ãŒå‹•è©ã‚’å«ã‚€æ–‡ç¯€ã«ä¿‚ã‚‹ã‚‚ã®ã‚’æŠ½å‡º
åè©ã‚’å«ã‚€æ–‡ç¯€ãŒï¼Œå‹•è©ã‚’å«ã‚€æ–‡ç¯€ã«ä¿‚ã‚‹ã¨ãï¼Œã“ã‚Œã‚‰ã‚’ã‚¿ãƒ–åŒºåˆ‡ã‚Šå½¢å¼ã§æŠ½å‡ºã›ã‚ˆï¼
ãŸã ã—ï¼Œå¥èª­ç‚¹ãªã©ã®è¨˜å·ã¯å‡ºåŠ›ã—ãªã„ã‚ˆã†ã«ã›ã‚ˆï¼
'''
for sentence in sentences:
    pairs: List[Tuple[Chunk, Chunk]] = relative_pair(chunk_list=sentence,
                                                     modifier_contains_pos=PartOfSpeech.NOUN,
                                                     modifiee_contains_pos=PartOfSpeech.VERB)
    for pair in pairs:
        print(pair[0].surface_words(need_syntax=True) + "\t" + pair[1].surface_words(need_syntax=True))

'''
44. ä¿‚ã‚Šå—ã‘æœ¨ã®å¯è¦–åŒ–
ä¸ãˆã‚‰ã‚ŒãŸæ–‡ã®ä¿‚ã‚Šå—ã‘æœ¨ã‚’æœ‰å‘ã‚°ãƒ©ãƒ•ã¨ã—ã¦å¯è¦–åŒ–ã›ã‚ˆï¼å¯è¦–åŒ–ã«ã¯ï¼Œä¿‚ã‚Šå—ã‘æœ¨ã‚’DOTè¨€èªã«å¤‰æ›ã—ï¼ŒGraphvizã‚’ç”¨ã„ã‚‹ã¨ã‚ˆã„ï¼
ã¾ãŸï¼ŒPythonã‹ã‚‰æœ‰å‘ã‚°ãƒ©ãƒ•ã‚’ç›´æ¥çš„ã«å¯è¦–åŒ–ã™ã‚‹ã«ã¯ï¼Œpydotã‚’ä½¿ã†ã¨ã‚ˆã„ï¼
'''

'''
graphvizã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦
 brew install graphviz
'''
from pydotplus import Dot, Node, Edge

dot: Dot = Dot(graph_type="digraph")
pairs: List[Tuple[Chunk, Chunk]] = relative_pair(chunk_list=sentences[10])

for modifier, modifiee in pairs[:-1]:
    node_modifier: Node = Node(modifier.surface_words())
    node_modifiee: Node = Node(modifiee.surface_words())
    dot.add_edge(Edge(node_modifier, node_modifiee))

dot.write(path="test.dot2.jpg", format="jpg")

'''
45. å‹•è©ã®æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
ä»Šå›ç”¨ã„ã¦ã„ã‚‹æ–‡ç« ã‚’ã‚³ãƒ¼ãƒ‘ã‚¹ã¨è¦‹ãªã—ï¼Œæ—¥æœ¬èªã®è¿°èªãŒå–ã‚Šã†ã‚‹æ ¼ã‚’èª¿æŸ»ã—ãŸã„ï¼
å‹•è©ã‚’è¿°èªï¼Œå‹•è©ã«ä¿‚ã£ã¦ã„ã‚‹æ–‡ç¯€ã®åŠ©è©ã‚’æ ¼ã¨è€ƒãˆï¼Œè¿°èªã¨æ ¼ã‚’ã‚¿ãƒ–åŒºåˆ‡ã‚Šå½¢å¼ã§å‡ºåŠ›ã›ã‚ˆï¼
ãŸã ã—ï¼Œå‡ºåŠ›ã¯ä»¥ä¸‹ã®ä»•æ§˜ã‚’æº€ãŸã™ã‚ˆã†ã«ã›ã‚ˆï¼

å‹•è©ã‚’å«ã‚€æ–‡ç¯€ã«ãŠã„ã¦ï¼Œæœ€å·¦ã®å‹•è©ã®åŸºæœ¬å½¢ã‚’è¿°èªã¨ã™ã‚‹
è¿°èªã«ä¿‚ã‚‹åŠ©è©ã‚’æ ¼ã¨ã™ã‚‹
è¿°èªã«ä¿‚ã‚‹åŠ©è©ï¼ˆæ–‡ç¯€ï¼‰ãŒè¤‡æ•°ã‚ã‚‹ã¨ãã¯ï¼Œã™ã¹ã¦ã®åŠ©è©ã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¾æ›¸é †ã«ä¸¦ã¹ã‚‹

ã€Œå¾è¼©ã¯ã“ã“ã§å§‹ã‚ã¦äººé–“ã¨ã„ã†ã‚‚ã®ã‚’è¦‹ãŸã€ã¨ã„ã†ä¾‹æ–‡ï¼ˆneko.txt.cabochaã®8æ–‡ç›®ï¼‰ã‚’è€ƒãˆã‚‹ï¼
ã“ã®æ–‡ã¯ã€Œå§‹ã‚ã‚‹ã€ã¨ã€Œè¦‹ã‚‹ã€ã®ï¼’ã¤ã®å‹•è©ã‚’å«ã¿ï¼Œ
ã€Œå§‹ã‚ã‚‹ã€ã«ä¿‚ã‚‹æ–‡ç¯€ã¯ã€Œã“ã“ã§ã€ï¼Œ
ã€Œè¦‹ã‚‹ã€ã«ä¿‚ã‚‹æ–‡ç¯€ã¯ã€Œå¾è¼©ã¯ã€ã¨ã€Œã‚‚ã®ã‚’ã€ã¨è§£æã•ã‚ŒãŸå ´åˆã¯ï¼Œ
æ¬¡ã®ã‚ˆã†ãªå‡ºåŠ›ã«ãªã‚‹ã¯ãšã§ã‚ã‚‹ï¼

å§‹ã‚ã‚‹  ã§
è¦‹ã‚‹    ã¯ ã‚’

ã‚³ãƒ¼ãƒ‘ã‚¹ä¸­ã§é »å‡ºã™ã‚‹è¿°èªã¨æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ„ã¿åˆã‚ã›
ã€Œã™ã‚‹ã€ã€Œè¦‹ã‚‹ã€ã€Œä¸ãˆã‚‹ã€ã¨ã„ã†å‹•è©ã®æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚³ãƒ¼ãƒ‘ã‚¹ä¸­ã§å‡ºç¾é »åº¦ã®é«˜ã„é †ã«ä¸¦ã¹ã‚ˆï¼‰
'''

lines: List[str] = []
for sentence in sentences:
    modifiee_modifiers_list: List[Tuple[Chunk, List[Chunk]]] = modifiee_modifiers(chunk_list=sentence,
                                                                                  modifier_contains_pos=PartOfSpeech.PARTICLE,
                                                                                  modifiee_contains_pos=PartOfSpeech.VERB)

    for modifiee_modifiers_ in modifiee_modifiers_list:
        verb: str = modifiee_modifiers_[0].base_word(pos=PartOfSpeech.VERB)
        particle_list: List[str] = map(lambda x: x.base_word(pos=PartOfSpeech.PARTICLE), modifiee_modifiers_[1])
        lines.append(verb + "\t" + " ".join(particle_list) + "\n")

with open("./chapter5/particle_pattern2.txt", "w") as file:
    file.writelines(lines)

'''
ã‚³ãƒ¼ãƒ‘ã‚¹ä¸­ã§é »å‡ºã™ã‚‹è¿°èªã¨æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ„ã¿åˆã‚ã›
'''
from collections import Counter

with open("./chapter5/particle_pattern2.txt", "r") as file:
    lines: List[str] = file.readlines()

counter: Counter = Counter(map(lambda x: x.strip("\n").replace("\t", " "), lines))
print(counter.most_common(10))

'''
ã€Œã™ã‚‹ã€ã€Œè¦‹ã‚‹ã€ã€Œä¸ãˆã‚‹ã€ã¨ã„ã†å‹•è©ã®æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚³ãƒ¼ãƒ‘ã‚¹ä¸­ã§å‡ºç¾é »åº¦ã®é«˜ã„é †ã«ä¸¦ã¹ã‚ˆï¼‰
'''
lines_suru: List[str] = filter(lambda x: x.split("\t")[0] == "ã™ã‚‹", lines)
lines_miru: List[str] = filter(lambda x: x.split("\t")[0] == "è¦‹ã‚‹", lines)
lines_ataeru: List[str] = filter(lambda x: x.split("\t")[0] == "ä¸ãˆã‚‹", lines)

counter_suru: Counter = Counter(map(lambda x: x.strip("\n").replace("\t", " "), lines_suru))
counter_miru: Counter = Counter(map(lambda x: x.strip("\n").replace("\t", " "), lines_miru))
counter_ataeru: Counter = Counter(map(lambda x: x.strip("\n").replace("\t", " "), lines_ataeru))

print(counter_suru.most_common(10))
print(counter_miru.most_common(10))
print(counter_ataeru.most_common(10))

'''
46. å‹•è©ã®æ ¼ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±ã®æŠ½å‡º
45ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ”¹å¤‰ã—ï¼Œè¿°èªã¨æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç¶šã‘ã¦é …ï¼ˆè¿°èªã«ä¿‚ã£ã¦ã„ã‚‹æ–‡ç¯€ãã®ã‚‚ã®ï¼‰ã‚’ã‚¿ãƒ–åŒºåˆ‡ã‚Šå½¢å¼ã§å‡ºåŠ›ã›ã‚ˆï¼
45ã®ä»•æ§˜ã«åŠ ãˆã¦ï¼Œä»¥ä¸‹ã®ä»•æ§˜ã‚’æº€ãŸã™ã‚ˆã†ã«ã›ã‚ˆï¼

é …ã¯è¿°èªã«ä¿‚ã£ã¦ã„ã‚‹æ–‡ç¯€ã®å˜èªåˆ—ã¨ã™ã‚‹ï¼ˆæœ«å°¾ã®åŠ©è©ã‚’å–ã‚Šé™¤ãå¿…è¦ã¯ãªã„ï¼‰
è¿°èªã«ä¿‚ã‚‹æ–‡ç¯€ãŒè¤‡æ•°ã‚ã‚‹ã¨ãã¯ï¼ŒåŠ©è©ã¨åŒä¸€ã®åŸºæº–ãƒ»é †åºã§ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§ä¸¦ã¹ã‚‹
ã€Œå¾è¼©ã¯ã“ã“ã§å§‹ã‚ã¦äººé–“ã¨ã„ã†ã‚‚ã®ã‚’è¦‹ãŸã€ã¨ã„ã†ä¾‹æ–‡ï¼ˆneko.txt.cabochaã®8æ–‡ç›®ï¼‰ã‚’è€ƒãˆã‚‹ï¼
ã“ã®æ–‡ã¯ã€Œå§‹ã‚ã‚‹ã€ã¨ã€Œè¦‹ã‚‹ã€ã®ï¼’ã¤ã®å‹•è©ã‚’å«ã¿ï¼Œ
ã€Œå§‹ã‚ã‚‹ã€ã«ä¿‚ã‚‹æ–‡ç¯€ã¯ã€Œã“ã“ã§ã€ï¼Œã€Œè¦‹ã‚‹ã€ã«ä¿‚ã‚‹æ–‡ç¯€ã¯ã€Œå¾è¼©ã¯ã€ã¨ã€Œã‚‚ã®ã‚’ã€ã¨è§£æã•ã‚ŒãŸå ´åˆã¯ï¼Œ
æ¬¡ã®ã‚ˆã†ãªå‡ºåŠ›ã«ãªã‚‹ã¯ãšã§ã‚ã‚‹ï¼

å§‹ã‚ã‚‹  ã§      ã“ã“ã§
è¦‹ã‚‹    ã¯ ã‚’   å¾è¼©ã¯ ã‚‚ã®ã‚’
'''
modifiee_modifiers_list: List[Tuple[Chunk, List[Chunk]]] = modifiee_modifiers(chunk_list=sentences[10],
                                                                              modifier_contains_pos=PartOfSpeech.PARTICLE,
                                                                              modifiee_contains_pos=PartOfSpeech.VERB)

for modifiee_modifiers_ in modifiee_modifiers_list:
    verb: str = modifiee_modifiers_[0].base_word(pos=PartOfSpeech.VERB)
    particle: str = " ".join(map(lambda x: x.base_word(pos=PartOfSpeech.PARTICLE), modifiee_modifiers_[1]))
    part: str = " ".join(map(lambda x: x.surface_words(), modifiee_modifiers_[1]))

    print(verb + "\t" + particle + "\t" + part)

'''
47. æ©Ÿèƒ½å‹•è©æ§‹æ–‡ã®ãƒã‚¤ãƒ‹ãƒ³ã‚°
å‹•è©ã®ãƒ²æ ¼ã«ã‚µå¤‰æ¥ç¶šåè©ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆã®ã¿ã«ç€ç›®ã—ãŸã„ï¼46ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä»¥ä¸‹ã®ä»•æ§˜ã‚’æº€ãŸã™ã‚ˆã†ã«æ”¹å¤‰ã›ã‚ˆï¼

ã€Œã‚µå¤‰æ¥ç¶šåè©+ã‚’ï¼ˆåŠ©è©ï¼‰ã€ã§æ§‹æˆã•ã‚Œã‚‹æ–‡ç¯€ãŒå‹•è©ã«ä¿‚ã‚‹å ´åˆã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
è¿°èªã¯ã€Œã‚µå¤‰æ¥ç¶šåè©+ã‚’+å‹•è©ã®åŸºæœ¬å½¢ã€ã¨ã—ï¼Œæ–‡ç¯€ä¸­ã«è¤‡æ•°ã®å‹•è©ãŒã‚ã‚‹ã¨ãã¯ï¼Œæœ€å·¦ã®å‹•è©ã‚’ç”¨ã„ã‚‹
è¿°èªã«ä¿‚ã‚‹åŠ©è©ï¼ˆæ–‡ç¯€ï¼‰ãŒè¤‡æ•°ã‚ã‚‹ã¨ãã¯ï¼Œã™ã¹ã¦ã®åŠ©è©ã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¾æ›¸é †ã«ä¸¦ã¹ã‚‹
è¿°èªã«ä¿‚ã‚‹æ–‡ç¯€ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ï¼Œã™ã¹ã¦ã®é …ã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§ä¸¦ã¹ã‚‹ï¼ˆåŠ©è©ã®ä¸¦ã³é †ã¨æƒãˆã‚ˆï¼‰
ä¾‹ãˆã°ã€Œåˆ¥æ®µãã‚‹ã«ã‚‚åŠã°ã‚“ã•ã¨ã€ä¸»äººã¯æ‰‹ç´™ã«è¿”äº‹ã‚’ã™ã‚‹ã€‚ã€ã¨ã„ã†æ–‡ã‹ã‚‰ï¼Œä»¥ä¸‹ã®å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹ã¯ãšã§ã‚ã‚‹ï¼

è¿”äº‹ã‚’ã™ã‚‹      ã¨ ã« ã¯        åŠã°ã‚“ã•ã¨ æ‰‹ç´™ã« ä¸»äººã¯

ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å‡ºåŠ›ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ï¼Œä»¥ä¸‹ã®äº‹é …ã‚’UNIXã‚³ãƒãƒ³ãƒ‰ã‚’ç”¨ã„ã¦ç¢ºèªã›ã‚ˆï¼
ã‚³ãƒ¼ãƒ‘ã‚¹ä¸­ã§é »å‡ºã™ã‚‹è¿°èªï¼ˆã‚µå¤‰æ¥ç¶šåè©+ã‚’+å‹•è©ï¼‰
ã‚³ãƒ¼ãƒ‘ã‚¹ä¸­ã§é »å‡ºã™ã‚‹è¿°èªã¨åŠ©è©ãƒ‘ã‚¿ãƒ¼ãƒ³
'''

predicate_list: List[str] = []
for sentence in sentences:
    modifiee_modifiers_list: List[Tuple[Chunk, List[Chunk]]] = modifiee_modifiers(chunk_list=sentence,
                                                                                  modifier_contains_pos=PartOfSpeech.PARTICLE,
                                                                                  modifiee_contains_pos=PartOfSpeech.VERB)

    for modifiee_modifiers_ in modifiee_modifiers_list:

        modifier_list: List[Chunk] = modifiee_modifiers_[1]
        chunk_sahen: Chunk = None
        for chunk in modifier_list:
            # ã‚µå¤‰æ¥ç¶š åè©ã®å½¢æ…‹ç´ ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹
            if "ã‚µå¤‰æ¥ç¶š" not in map(lambda x: x.pos1, chunk.morph_list):
                continue
            # åŠ©è©ãŒãƒ²æ ¼ã‹ã©ã†ã‹
            if "ã‚’" not in map(lambda x: x.base, chunk.morph_list):
                continue
            chunk_sahen = chunk

        # ã‚‚ã—ä¿‚ã‚Šå…ƒã«ã‚µå¤‰æ¥ç¶šåè©ãŒãªã‹ã£ãŸã‚‰æ¬¡ã‚’æ¢ã™
        if not chunk_sahen:
            continue

        modifier_list.remove(chunk_sahen)

        predicate: str = chunk_sahen.surface_words() + modifiee_modifiers_[0].base_word(pos=PartOfSpeech.VERB)
        particle: str = " ".join(map(lambda x: x.base_word(pos=PartOfSpeech.PARTICLE), modifiee_modifiers_[1]))
        part: str = " ".join(map(lambda x: x.surface_words(), modifiee_modifiers_[1]))

        print("ã•ã¸ã‚“: " + predicate + "\t" + particle + "\t" + part)

        # é »å‡ºã™ã‚‹è¿°èªã‚’èª¿ã¹ã‚‹
        predicate_list.append(predicate)

counter: Counter = Counter(predicate_list)
print(counter.most_common(100))


'''
48. åè©ã‹ã‚‰æ ¹ã¸ã®ãƒ‘ã‚¹ã®æŠ½å‡º
æ–‡ä¸­ã®ã™ã¹ã¦ã®åè©ã‚’å«ã‚€æ–‡ç¯€ã«å¯¾ã—ï¼Œãã®æ–‡ç¯€ã‹ã‚‰æ§‹æ–‡æœ¨ã®æ ¹ã«è‡³ã‚‹ãƒ‘ã‚¹ã‚’æŠ½å‡ºã›ã‚ˆï¼ 
ãŸã ã—ï¼Œæ§‹æ–‡æœ¨ä¸Šã®ãƒ‘ã‚¹ã¯ä»¥ä¸‹ã®ä»•æ§˜ã‚’æº€ãŸã™ã‚‚ã®ã¨ã™ã‚‹ï¼

å„æ–‡ç¯€ã¯ï¼ˆè¡¨å±¤å½¢ã®ï¼‰å½¢æ…‹ç´ åˆ—ã§è¡¨ç¾ã™ã‚‹
ãƒ‘ã‚¹ã®é–‹å§‹æ–‡ç¯€ã‹ã‚‰çµ‚äº†æ–‡ç¯€ã«è‡³ã‚‹ã¾ã§ï¼Œå„æ–‡ç¯€ã®è¡¨ç¾ã‚’"->"ã§é€£çµã™ã‚‹
ã€Œå¾è¼©ã¯ã“ã“ã§å§‹ã‚ã¦äººé–“ã¨ã„ã†ã‚‚ã®ã‚’è¦‹ãŸã€ã¨ã„ã†æ–‡ï¼ˆneko.txt.cabochaã®8æ–‡ç›®ï¼‰ã‹ã‚‰ï¼Œæ¬¡ã®ã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹ã¯ãšã§ã‚ã‚‹ï¼

å¾è¼©ã¯ -> è¦‹ãŸ
ã“ã“ã§ -> å§‹ã‚ã¦ -> äººé–“ã¨ã„ã† -> ã‚‚ã®ã‚’ -> è¦‹ãŸ
äººé–“ã¨ã„ã† -> ã‚‚ã®ã‚’ -> è¦‹ãŸ
ã‚‚ã®ã‚’ -> è¦‹ãŸ
'''

for sentence in sentences:

    for chunk in sentence:
        if not chunk.is_contain(pos=PartOfSpeech.NOUN):
            continue

        path: str = chunk.surface_words()
        modifiee_number: int = chunk.modifiee_number()
        while modifiee_number != -1:
            next_chunk: Chunk = sentence[modifiee_number]
            path += " -> " + next_chunk.surface_words()
            modifiee_number = next_chunk.modifiee_number()
        print(path)

    print()






